# Cross-Lingual Translation Using Deep Learning and NLP

This repository contains a complete cross-lingual machine translation system built using multiple deep learning architectures. The project explores various Neural Machine Translation (NMT) models, with IndicTrans2 being the primary and most effective model used for multilingual translation‚Äîespecially for Indian and low-resource languages.

The goal is to implement an end-to-end translation pipeline capable of translating text across different languages using state-of-the-art NLP and deep learning techniques.



üöÄ **Models Used**

1Ô∏è‚É£ **IndicTrans2 (Primary Model)**

1.Transformer-based multilingual NMT model

2.Highly optimized for Indian and low-resource languages

3.Provides state-of-the-art accuracy and fluency

4.Used as the main translation engine in this project



2Ô∏è‚É£ **Transformer Model**

1.Standard encoder‚Äìdecoder architecture

2.Serves as a baseline to compare accuracy and BLEU scores



3Ô∏è‚É£ **Seq2Seq with Attention**

1.Classic NMT architecture

2.Implemented for understanding performance differences



üåü **Key Features**

1.Complete multilingual translation pipeline

2.Tokenization using SentencePiece

3.Dataset preprocessing and normalization

4.Training and fine-tuning workflows

5.Evaluation using BLEU, accuracy, perplexity

6.Real-time inference using IndicTrans2

7.Model comparison with visualized performance metrics


üéØ **Objectives**

1.Build a robust multilingual machine translation system

2.Compare multiple NMT architectures

3.Highlight the effectiveness of IndicTrans2

4.Provide a ready-to-use translation interface



üõ†Ô∏è **Technologies Used**

1.IndicTrans2 (Facebook AI Research)

2.PyTorch

3.Python

4.Transformer architecture

5.Seq2Seq with Attention

6.SentencePiece Tokenizer

7.NumPy, Pandas

8.BLEU Score Evaluation

9.NLP preprocessing libraries
